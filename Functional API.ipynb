{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2b3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataP.csv')\n",
    "df['urgency'] = df['urgency'].fillna(0)\n",
    "texts = df[\"letter_text\"].astype(str).values\n",
    "labels_type = df[\"referal_type\"].astype(str).values\n",
    "labels_urgency = df[\"urgency\"].astype(str).values\n",
    "#Split the data\n",
    "X = df['letter_text']\n",
    "y = df[['referal_type', 'urgency']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd0173a",
   "metadata": {},
   "source": [
    "# Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de8423d",
   "metadata": {},
   "source": [
    "## Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74798450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding\n",
    "label_encoder_type = LabelEncoder()\n",
    "labels_type_encoded = label_encoder_type.fit_transform(labels_type)\n",
    "num_classes_type = len(label_encoder_type.classes_)\n",
    "\n",
    "label_encoder_urgency = LabelEncoder()\n",
    "labels_urgency_encoded = label_encoder_urgency.fit_transform(labels_urgency)\n",
    "num_classes_urgency = len(label_encoder_urgency.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3661af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenisation and Sequencing\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "max_len = max(len(s) for s in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b927c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test split\n",
    "X_train, X_test, y_train_type, y_test_type, y_train_urgency, y_test_urgency = train_test_split(\n",
    "    padded_sequences,\n",
    "    labels_type_encoded,\n",
    "    labels_urgency_encoded,\n",
    "    test_size=0.2,\n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7ef248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input layer\n",
    "inputs = keras.Input(shape=(max_len,))\n",
    "\n",
    "# Shared layers - the core of the model\n",
    "embedding_layer = keras.layers.Embedding(max_words, 128)(inputs)\n",
    "pooling_layer = keras.layers.GlobalAveragePooling1D()(embedding_layer)\n",
    "\n",
    "# Add dropout for regularization\n",
    "dropout_1 = keras.layers.Dropout(0.5)(pooling_layer)\n",
    "\n",
    "# Dense layer with L2 regularization to prevent overfitting\n",
    "shared_dense_layer = keras.layers.Dense(\n",
    "    32, \n",
    "    activation=\"relu\",\n",
    "    kernel_regularizer=l2(0.001)\n",
    ")(dropout_1)\n",
    "\n",
    "# Additional dropout before output layers\n",
    "dropout_2 = keras.layers.Dropout(0.3)(shared_dense_layer)\n",
    "\n",
    "# Output layer for referral type prediction\n",
    "type_output = keras.layers.Dense(\n",
    "    num_classes_type, activation=\"softmax\", name=\"type_output\"\n",
    ")(dropout_2)\n",
    "\n",
    "# Output layer for urgency prediction\n",
    "urgency_output = keras.layers.Dense(\n",
    "    num_classes_urgency, activation=\"softmax\", name=\"urgency_output\"\n",
    ")(dropout_2)\n",
    "\n",
    "# Build the complete model\n",
    "model = keras.Model(inputs=inputs, outputs=[type_output, urgency_output])\n",
    "\n",
    "# Use Adam optimizer with lower learning rate\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "\n",
    "# Compile with different loss weights for each task\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={\n",
    "        \"type_output\": \"sparse_categorical_crossentropy\",\n",
    "        \"urgency_output\": \"sparse_categorical_crossentropy\"\n",
    "    },\n",
    "    loss_weights={\n",
    "        \"type_output\": 0.4,      # Less weight for type prediction\n",
    "        \"urgency_output\": 1.6    # More weight for urgency (harder task)\n",
    "    },\n",
    "    metrics={\n",
    "        \"type_output\": \"accuracy\",\n",
    "        \"urgency_output\": \"accuracy\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set up early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y={\"type_output\": y_train_type, \"urgency_output\": y_train_urgency},\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, {\"type_output\": y_test_type, \"urgency_output\": y_test_urgency}),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734765ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "#Plot for Referral Type\n",
    "type_acc = history_dict['type_output_accuracy']\n",
    "val_type_acc = history_dict['val_type_output_accuracy']\n",
    "type_loss = history_dict['type_output_loss']\n",
    "val_type_loss = history_dict['val_type_output_loss']\n",
    "epochs = range(1, len(type_acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Type Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, type_acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_type_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy (Referral Type)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Type Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, type_loss, 'ro', label='Training loss')\n",
    "plt.plot(epochs, val_type_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss (Referral Type)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle('Performance for Referral Type Output', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot for Urgency\n",
    "urgency_acc = history_dict['urgency_output_accuracy']\n",
    "val_urgency_acc = history_dict['val_urgency_output_accuracy']\n",
    "urgency_loss = history_dict['urgency_output_loss']\n",
    "val_urgency_loss = history_dict['val_urgency_output_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Urgency Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, urgency_acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_urgency_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy (Urgency)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Urgency Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, urgency_loss, 'ro', label='Training loss')\n",
    "plt.plot(epochs, val_urgency_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss (Urgency)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle('Performance for Urgency Output', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# predictions for the test set.\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Find highest probability for each output.\n",
    "predicted_labels_type = np.argmax(predictions[0], axis=1)\n",
    "predicted_labels_urgency = np.argmax(predictions[1], axis=1)\n",
    "\n",
    "# Classifciation report for each output.\n",
    "\n",
    "print(\"Final Performance Report on Test Data\")\n",
    "\n",
    "print(\"\\nClassification Report for 'Referral Type'\")\n",
    "print(classification_report(\n",
    "    y_test_type,\n",
    "    predicted_labels_type,\n",
    "    target_names=label_encoder_type.classes_,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "print(\"\\nClassification Report for 'Urgency'\")\n",
    "print(classification_report(\n",
    "    y_test_urgency,\n",
    "    predicted_labels_urgency,\n",
    "    target_names=label_encoder_urgency.classes_,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# Confusion Matrix for 'Type'\n",
    "cm_type = confusion_matrix(y_test_type, predicted_labels_type)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_type, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder_type.classes_,\n",
    "            yticklabels=label_encoder_type.classes_)\n",
    "plt.title(\"Confusion Matrix: Referral Type\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix for 'Urgency'\n",
    "cm_urgency = confusion_matrix(y_test_urgency, predicted_labels_urgency)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_urgency, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=label_encoder_urgency.classes_,\n",
    "            yticklabels=label_encoder_urgency.classes_)\n",
    "plt.title(\"Confusion Matrix: Urgency\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d6e41",
   "metadata": {},
   "source": [
    "## 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to collect scores\n",
    "f1_scores_type = []\n",
    "f1_scores_urgency = []\n",
    "acc_scores_type = []\n",
    "acc_scores_urgency = []\n",
    "\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(X):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "    fold += 1\n",
    "\n",
    "    # Split data\n",
    "    X_train_fold = X.iloc[train_index]\n",
    "    X_val_fold = X.iloc[val_index]\n",
    "    y_train_type_fold = y['referal_type'].iloc[train_index]\n",
    "    y_train_urgency_fold = y['urgency'].iloc[train_index]\n",
    "    y_val_type_fold = y['referal_type'].iloc[val_index]\n",
    "    y_val_urgency_fold = y['urgency'].iloc[val_index]\n",
    "\n",
    "    # Create new label encoders for each fold\n",
    "    label_encoder_type_fold = LabelEncoder()\n",
    "    label_encoder_urgency_fold = LabelEncoder()\n",
    "\n",
    "    # Fit on training data only\n",
    "    label_encoder_type_fold.fit(y_train_type_fold)\n",
    "    label_encoder_urgency_fold.fit(y_train_urgency_fold)\n",
    "\n",
    "    # Update num_classes for this fold\n",
    "    num_classes_type_fold = len(label_encoder_type_fold.classes_)\n",
    "    num_classes_urgency_fold = len(label_encoder_urgency_fold.classes_)\n",
    "\n",
    "    # Tokenize and pad\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train_fold)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val_fold)\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=max_len)\n",
    "\n",
    "    # Build model\n",
    "    inputs = keras.Input(shape=(max_len,))\n",
    "    embedding_layer = keras.layers.Embedding(max_words, 128)(inputs)\n",
    "    pooling_layer = keras.layers.GlobalAveragePooling1D()(embedding_layer)\n",
    "    dropout_1 = keras.layers.Dropout(0.5)(pooling_layer)\n",
    "    shared_dense_layer = keras.layers.Dense(\n",
    "        32, activation=\"relu\", kernel_regularizer=l2(0.001)\n",
    "    )(dropout_1)\n",
    "    dropout_2 = keras.layers.Dropout(0.3)(shared_dense_layer)\n",
    "\n",
    "    type_output = keras.layers.Dense(num_classes_type_fold, activation=\"softmax\", name=\"type_output\")(dropout_2)\n",
    "    urgency_output = keras.layers.Dense(num_classes_urgency_fold, activation=\"softmax\", name=\"urgency_output\")(dropout_2)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=[type_output, urgency_output])\n",
    "\n",
    "    # Compile model\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss={\n",
    "            \"type_output\": \"sparse_categorical_crossentropy\",\n",
    "            \"urgency_output\": \"sparse_categorical_crossentropy\"\n",
    "        },\n",
    "        loss_weights={\n",
    "            \"type_output\": 0.4,\n",
    "            \"urgency_output\": 1.6\n",
    "        },\n",
    "        metrics={\n",
    "            \"type_output\": \"accuracy\",\n",
    "            \"urgency_output\": \"accuracy\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    # Encode string labels into integers using fold-specific encoders\n",
    "    y_train_type_fold = label_encoder_type_fold.transform(y_train_type_fold)\n",
    "    y_train_urgency_fold = label_encoder_urgency_fold.transform(y_train_urgency_fold)\n",
    "    y_val_type_fold = label_encoder_type_fold.transform(y_val_type_fold)\n",
    "    y_val_urgency_fold = label_encoder_urgency_fold.transform(y_val_urgency_fold)\n",
    "\n",
    "    y_train_type_fold = np.array(y_train_type_fold).astype('int32')\n",
    "    y_train_urgency_fold = np.array(y_train_urgency_fold).astype('int32')\n",
    "    y_val_type_fold = np.array(y_val_type_fold).astype('int32')\n",
    "    y_val_urgency_fold = np.array(y_val_urgency_fold).astype('int32')\n",
    "\n",
    "    # Train\n",
    "    model.fit(\n",
    "        X_train_pad,\n",
    "        {\"type_output\": y_train_type_fold, \"urgency_output\": y_train_urgency_fold},\n",
    "        validation_data=(X_val_pad, {\"type_output\": y_val_type_fold, \"urgency_output\": y_val_urgency_fold}),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(X_val_pad)\n",
    "    pred_type = np.argmax(predictions[0], axis=1)\n",
    "    pred_urgency = np.argmax(predictions[1], axis=1)\n",
    "\n",
    "    # Evaluate\n",
    "    f1_type = f1_score(y_val_type_fold, pred_type, average='macro')\n",
    "    f1_urgency = f1_score(y_val_urgency_fold, pred_urgency, average='macro')\n",
    "    acc_type = accuracy_score(y_val_type_fold, pred_type)\n",
    "    acc_urgency = accuracy_score(y_val_urgency_fold, pred_urgency)\n",
    "\n",
    "   \n",
    "    f1_scores_type.append(f1_type)\n",
    "    f1_scores_urgency.append(f1_urgency)\n",
    "    acc_scores_type.append(acc_type)\n",
    "    acc_scores_urgency.append(acc_urgency)\n",
    "\n",
    "    print(f\"Referral Type → F1: {f1_type:.4f}, Accuracy: {acc_type:.4f}\")\n",
    "    print(f\"Urgency       → F1: {f1_urgency:.4f}, Accuracy: {acc_urgency:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nK-Fold Final Results\")\n",
    "print(\"Referral Type F1 scores:\", f1_scores_type)\n",
    "print(\"Referral Type Acc scores:\", acc_scores_type)\n",
    "print(f\"Referral Type - Mean F1: {np.mean(f1_scores_type):.4f}, Mean Acc: {np.mean(acc_scores_type):.4f}\")\n",
    "\n",
    "print(\"\\nUrgency F1 scores:\", f1_scores_urgency)\n",
    "print(\"Urgency Acc scores:\", acc_scores_urgency)\n",
    "print(f\"Urgency - Mean F1: {np.mean(f1_scores_urgency):.4f}, Mean Acc: {np.mean(acc_scores_urgency):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e82b8d",
   "metadata": {},
   "source": [
    "## Vascular-Only Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb597600",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataP.csv')\n",
    "df = df[df['referal_type'] != 'non vascular']\n",
    "df['referal_type'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b607fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[\"letter_text\"].astype(str).values\n",
    "labels_type = df[\"referal_type\"].astype(str).values\n",
    "labels_urgency = df[\"urgency\"].astype(str).values\n",
    "\n",
    "X = df['letter_text']\n",
    "y = df[['referal_type', 'urgency']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46624e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding\n",
    "label_encoder_type = LabelEncoder()\n",
    "labels_type_encoded = label_encoder_type.fit_transform(labels_type)\n",
    "num_classes_type = len(label_encoder_type.classes_)\n",
    "\n",
    "label_encoder_urgency = LabelEncoder()\n",
    "labels_urgency_encoded = label_encoder_urgency.fit_transform(labels_urgency)\n",
    "num_classes_urgency = len(label_encoder_urgency.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenisation and Sequencing\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "max_len = max(len(s) for s in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94b0ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test split\n",
    "X_train, X_test, y_train_type, y_test_type, y_train_urgency, y_test_urgency = train_test_split(\n",
    "    padded_sequences,\n",
    "    labels_type_encoded,\n",
    "    labels_urgency_encoded,\n",
    "    test_size=0.2,\n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93845986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input layer\n",
    "inputs = keras.Input(shape=(max_len,))\n",
    "\n",
    "# Shared layers - the core of the model\n",
    "embedding_layer = keras.layers.Embedding(max_words, 128)(inputs)\n",
    "pooling_layer = keras.layers.GlobalAveragePooling1D()(embedding_layer)\n",
    "\n",
    "# Add dropout for regularization\n",
    "dropout_1 = keras.layers.Dropout(0.5)(pooling_layer)\n",
    "\n",
    "# Dense layer with L2 regularization to prevent overfitting\n",
    "shared_dense_layer = keras.layers.Dense(\n",
    "    32,  \n",
    "    activation=\"relu\",\n",
    "    kernel_regularizer=l2(0.001)\n",
    ")(dropout_1)\n",
    "\n",
    "# Additional dropout before output layers\n",
    "dropout_2 = keras.layers.Dropout(0.3)(shared_dense_layer)\n",
    "\n",
    "# Output layer for referral type prediction\n",
    "type_output = keras.layers.Dense(\n",
    "    num_classes_type, activation=\"softmax\", name=\"type_output\"\n",
    ")(dropout_2)\n",
    "\n",
    "# Output layer for urgency prediction\n",
    "urgency_output = keras.layers.Dense(\n",
    "    num_classes_urgency, activation=\"softmax\", name=\"urgency_output\"\n",
    ")(dropout_2)\n",
    "\n",
    "# Build the complete model\n",
    "model = keras.Model(inputs=inputs, outputs=[type_output, urgency_output])\n",
    "\n",
    "# Use Adam optimizer with lower learning rate\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "\n",
    "# Compile with different loss weights for each task\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={\n",
    "        \"type_output\": \"sparse_categorical_crossentropy\",\n",
    "        \"urgency_output\": \"sparse_categorical_crossentropy\"\n",
    "    },\n",
    "    loss_weights={\n",
    "        \"type_output\": 0.4,      # Less weight for type prediction\n",
    "        \"urgency_output\": 1.6    # More weight for urgency (harder task)\n",
    "    },\n",
    "    metrics={\n",
    "        \"type_output\": \"accuracy\",\n",
    "        \"urgency_output\": \"accuracy\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set up early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y={\"type_output\": y_train_type, \"urgency_output\": y_train_urgency},\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, {\"type_output\": y_test_type, \"urgency_output\": y_test_urgency}),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d9889",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "#Plot for Referral Type\n",
    "type_acc = history_dict['type_output_accuracy']\n",
    "val_type_acc = history_dict['val_type_output_accuracy']\n",
    "type_loss = history_dict['type_output_loss']\n",
    "val_type_loss = history_dict['val_type_output_loss']\n",
    "epochs = range(1, len(type_acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Type Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, type_acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_type_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy (Referral Type)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Type Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, type_loss, 'ro', label='Training loss')\n",
    "plt.plot(epochs, val_type_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss (Referral Type)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle('Performance for Referral Type Output', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot for Urgency\n",
    "urgency_acc = history_dict['urgency_output_accuracy']\n",
    "val_urgency_acc = history_dict['val_urgency_output_accuracy']\n",
    "urgency_loss = history_dict['urgency_output_loss']\n",
    "val_urgency_loss = history_dict['val_urgency_output_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Urgency Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, urgency_acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_urgency_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy (Urgency)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Urgency Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, urgency_loss, 'ro', label='Training loss')\n",
    "plt.plot(epochs, val_urgency_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss (Urgency)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle('Performance for Urgency Output', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# predictions for the test set.\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Find highest probability for each output.\n",
    "predicted_labels_type = np.argmax(predictions[0], axis=1)\n",
    "predicted_labels_urgency = np.argmax(predictions[1], axis=1)\n",
    "\n",
    "# Classifciation report for each output.\n",
    "\n",
    "print(\"Final Performance Report on Test Data\")\n",
    "\n",
    "print(\"\\nClassification Report for 'Referral Type'\")\n",
    "print(classification_report(\n",
    "    y_test_type,\n",
    "    predicted_labels_type,\n",
    "    target_names=label_encoder_type.classes_,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "print(\"\\nClassification Report for 'Urgency'\")\n",
    "print(classification_report(\n",
    "    y_test_urgency,\n",
    "    predicted_labels_urgency,\n",
    "    target_names=label_encoder_urgency.classes_,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# Confusion Matrix for 'Type'\n",
    "cm_type = confusion_matrix(y_test_type, predicted_labels_type)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_type, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder_type.classes_,\n",
    "            yticklabels=label_encoder_type.classes_)\n",
    "plt.title(\"Confusion Matrix: Referral Type\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix for 'Urgency'\n",
    "cm_urgency = confusion_matrix(y_test_urgency, predicted_labels_urgency)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_urgency, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=label_encoder_urgency.classes_,\n",
    "            yticklabels=label_encoder_urgency.classes_)\n",
    "plt.title(\"Confusion Matrix: Urgency\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7900baba",
   "metadata": {},
   "source": [
    "## 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda41f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to collect scores\n",
    "f1_scores_type = []\n",
    "f1_scores_urgency = []\n",
    "acc_scores_type = []\n",
    "acc_scores_urgency = []\n",
    "\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(X):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "    fold += 1\n",
    "\n",
    "    # Split data\n",
    "    X_train_fold = X.iloc[train_index]\n",
    "    X_val_fold = X.iloc[val_index]\n",
    "    y_train_type_fold = y['referal_type'].iloc[train_index]\n",
    "    y_train_urgency_fold = y['urgency'].iloc[train_index]\n",
    "    y_val_type_fold = y['referal_type'].iloc[val_index]\n",
    "    y_val_urgency_fold = y['urgency'].iloc[val_index]\n",
    "\n",
    "    # Create new label encoders for each fold\n",
    "    label_encoder_type_fold = LabelEncoder()\n",
    "    label_encoder_urgency_fold = LabelEncoder()\n",
    "\n",
    "    # Fit on training data only\n",
    "    label_encoder_type_fold.fit(y_train_type_fold)\n",
    "    label_encoder_urgency_fold.fit(y_train_urgency_fold)\n",
    "\n",
    "    # Update num_classes for this fold\n",
    "    num_classes_type_fold = len(label_encoder_type_fold.classes_)\n",
    "    num_classes_urgency_fold = len(label_encoder_urgency_fold.classes_)\n",
    "\n",
    "    # Tokenize and pad\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train_fold)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val_fold)\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=max_len)\n",
    "\n",
    "    # Build model\n",
    "    inputs = keras.Input(shape=(max_len,))\n",
    "    embedding_layer = keras.layers.Embedding(max_words, 128)(inputs)\n",
    "    pooling_layer = keras.layers.GlobalAveragePooling1D()(embedding_layer)\n",
    "    dropout_1 = keras.layers.Dropout(0.5)(pooling_layer)\n",
    "    shared_dense_layer = keras.layers.Dense(\n",
    "        32, activation=\"relu\", kernel_regularizer=l2(0.001)\n",
    "    )(dropout_1)\n",
    "    dropout_2 = keras.layers.Dropout(0.3)(shared_dense_layer)\n",
    "\n",
    "    type_output = keras.layers.Dense(num_classes_type_fold, activation=\"softmax\", name=\"type_output\")(dropout_2)\n",
    "    urgency_output = keras.layers.Dense(num_classes_urgency_fold, activation=\"softmax\", name=\"urgency_output\")(dropout_2)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=[type_output, urgency_output])\n",
    "\n",
    "    # Compile model\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss={\n",
    "            \"type_output\": \"sparse_categorical_crossentropy\",\n",
    "            \"urgency_output\": \"sparse_categorical_crossentropy\"\n",
    "        },\n",
    "        loss_weights={\n",
    "            \"type_output\": 0.4,\n",
    "            \"urgency_output\": 1.6\n",
    "        },\n",
    "        metrics={\n",
    "            \"type_output\": \"accuracy\",\n",
    "            \"urgency_output\": \"accuracy\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    # Encode string labels into integers using fold-specific encoders\n",
    "    y_train_type_fold = label_encoder_type_fold.transform(y_train_type_fold)\n",
    "    y_train_urgency_fold = label_encoder_urgency_fold.transform(y_train_urgency_fold)\n",
    "    y_val_type_fold = label_encoder_type_fold.transform(y_val_type_fold)\n",
    "    y_val_urgency_fold = label_encoder_urgency_fold.transform(y_val_urgency_fold)\n",
    "\n",
    "    y_train_type_fold = np.array(y_train_type_fold).astype('int32')\n",
    "    y_train_urgency_fold = np.array(y_train_urgency_fold).astype('int32')\n",
    "    y_val_type_fold = np.array(y_val_type_fold).astype('int32')\n",
    "    y_val_urgency_fold = np.array(y_val_urgency_fold).astype('int32')\n",
    "\n",
    "    # Train\n",
    "    model.fit(\n",
    "        X_train_pad,\n",
    "        {\"type_output\": y_train_type_fold, \"urgency_output\": y_train_urgency_fold},\n",
    "        validation_data=(X_val_pad, {\"type_output\": y_val_type_fold, \"urgency_output\": y_val_urgency_fold}),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(X_val_pad)\n",
    "    pred_type = np.argmax(predictions[0], axis=1)\n",
    "    pred_urgency = np.argmax(predictions[1], axis=1)\n",
    "\n",
    "    # Evaluate\n",
    "    f1_type = f1_score(y_val_type_fold, pred_type, average='macro')\n",
    "    f1_urgency = f1_score(y_val_urgency_fold, pred_urgency, average='macro')\n",
    "    acc_type = accuracy_score(y_val_type_fold, pred_type)\n",
    "    acc_urgency = accuracy_score(y_val_urgency_fold, pred_urgency)\n",
    "\n",
    "   \n",
    "    f1_scores_type.append(f1_type)\n",
    "    f1_scores_urgency.append(f1_urgency)\n",
    "    acc_scores_type.append(acc_type)\n",
    "    acc_scores_urgency.append(acc_urgency)\n",
    "\n",
    "    print(f\"Referral Type → F1: {f1_type:.4f}, Accuracy: {acc_type:.4f}\")\n",
    "    print(f\"Urgency       → F1: {f1_urgency:.4f}, Accuracy: {acc_urgency:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nK-Fold Final Results\")\n",
    "print(\"Referral Type F1 scores:\", f1_scores_type)\n",
    "print(\"Referral Type Acc scores:\", acc_scores_type)\n",
    "print(f\"Referral Type - Mean F1: {np.mean(f1_scores_type):.4f}, Mean Acc: {np.mean(acc_scores_type):.4f}\")\n",
    "\n",
    "print(\"\\nUrgency F1 scores:\", f1_scores_urgency)\n",
    "print(\"Urgency Acc scores:\", acc_scores_urgency)\n",
    "print(f\"Urgency - Mean F1: {np.mean(f1_scores_urgency):.4f}, Mean Acc: {np.mean(acc_scores_urgency):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
