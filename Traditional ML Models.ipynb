{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1e8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "#visualise performance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d04558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and sort dataset\n",
    "df = pd.read_csv('dataP.csv')\n",
    "df['urgency'] = df['urgency'].fillna(0)\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df['urgency'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3df96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#must be converted to strings so they cna be stored in the same array\n",
    "df['referal_type'] = df['referal_type'].astype(str)\n",
    "df['urgency'] = df['urgency'].astype(str)\n",
    "\n",
    "#Split the data\n",
    "X = df['letter_text']\n",
    "y = df[['referal_type', 'urgency']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25,\n",
    "                                                    random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a79adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def universal_ML_model(ML, **kwargs):\n",
    "  base_model = ML(**kwargs)\n",
    "\n",
    "  model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(base_model))\n",
    "    ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b17fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multi_output_model(model, X_test, y_test, label_names=['referal_type', 'urgency']):\n",
    "\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  y_pred_df = pd.DataFrame(y_pred, columns = label_names)\n",
    "\n",
    "  for label in label_names:\n",
    "    print(f\"\\n=== Classifcation Report: {label.capitalize()} ===\")\n",
    "    print(classification_report(y_test[label], y_pred_df[label], zero_division = 0))\n",
    "\n",
    "  fig, axes = plt.subplots(1, 2, figsize = (16, 6))\n",
    "  fig.suptitle('Model Performance Visualisation', fontsize = 16)\n",
    "\n",
    "  for i, label in enumerate(label_names):\n",
    "    cm = confusion_matrix(y_test[label], y_pred_df[label], labels = np.unique(y_test[label]))\n",
    "    sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'Blues' if i == 0 else 'Reds', ax = axes[i],\n",
    "                xticklabels = np.unique(y_test[label]), yticklabels = np.unique(y_test[label]))\n",
    "    axes[i].set_title(\"Confusion Matrix (Urgency)\")\n",
    "    axes[i].set_xlabel(\"Predicted Label\")\n",
    "    axes[i].set_ylabel(\"True Label\")\n",
    "\n",
    "  plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fffa17",
   "metadata": {},
   "source": [
    "# Full Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c821a",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c70909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the multi output model\n",
    "SVM = universal_ML_model(SVC, kernel = 'linear', probability = True, random_state = 1)\n",
    "evaluate_multi_output_model(SVM, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1ab774",
   "metadata": {},
   "source": [
    "### 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64f51ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare results\n",
    "f1_scores_type = []\n",
    "f1_scores_urgency = []\n",
    "acc_scores_type = []\n",
    "acc_scores_urgency = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    print(f\"\\nFold {fold}\")\n",
    "    fold += 1\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Train your pipeline\n",
    "    SVM.fit(X_train, y_train)\n",
    "    y_pred = SVM.predict(X_test)\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=y.columns)\n",
    "\n",
    "    # Compute F1 and Accuracy per output\n",
    "    f1_type = f1_score(y_test['referal_type'], y_pred_df['referal_type'], average='macro')\n",
    "    f1_urg = f1_score(y_test['urgency'], y_pred_df['urgency'], average='macro')\n",
    "    acc_type = accuracy_score(y_test['referal_type'], y_pred_df['referal_type'])\n",
    "    acc_urg = accuracy_score(y_test['urgency'], y_pred_df['urgency'])\n",
    "\n",
    "    f1_scores_type.append(f1_type)\n",
    "    f1_scores_urgency.append(f1_urg)\n",
    "    acc_scores_type.append(acc_type)\n",
    "    acc_scores_urgency.append(acc_urg)\n",
    "\n",
    "    print(f\"Referral Type → F1: {f1_type:.4f}, Acc: {acc_type:.4f}\")\n",
    "    print(f\"Urgency       → F1: {f1_urg:.4f}, Acc: {acc_urg:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nFinal SVM Results\")\n",
    "print(\"Referral Type F1:\", f1_scores_type)\n",
    "print(\"Referral Type Acc:\", acc_scores_type)\n",
    "print(f\"Mean F1: {np.mean(f1_scores_type):.4f}, Mean Acc: {np.mean(acc_scores_type):.4f}\")\n",
    "\n",
    "print(\"\\nUrgency F1:\", f1_scores_urgency)\n",
    "print(\"Urgency Acc:\", acc_scores_urgency)\n",
    "print(f\"Mean F1: {np.mean(f1_scores_urgency):.4f}, Mean Acc: {np.mean(acc_scores_urgency):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b9e953",
   "metadata": {},
   "source": [
    "## k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c525a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "kNN = universal_ML_model(KNeighborsClassifier, n_neighbors = 5)\n",
    "evaluate_multi_output_model(kNN, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b089f",
   "metadata": {},
   "source": [
    "### 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774d2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results\n",
    "f1_scores_type = []\n",
    "f1_scores_urgency = []\n",
    "acc_scores_type = []\n",
    "acc_scores_urgency = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "fold = 1\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    print(f\"\\nFold {fold}\")\n",
    "    fold += 1\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Train your pipeline\n",
    "    kNN.fit(X_train, y_train)\n",
    "    y_pred = kNN.predict(X_test)\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=y.columns)\n",
    "\n",
    "    # Compute F1 and Accuracy per output\n",
    "    f1_type = f1_score(y_test['referal_type'], y_pred_df['referal_type'], average='macro')\n",
    "    f1_urg = f1_score(y_test['urgency'], y_pred_df['urgency'], average='macro')\n",
    "    acc_type = accuracy_score(y_test['referal_type'], y_pred_df['referal_type'])\n",
    "    acc_urg = accuracy_score(y_test['urgency'], y_pred_df['urgency'])\n",
    "\n",
    "    f1_scores_type.append(f1_type)\n",
    "    f1_scores_urgency.append(f1_urg)\n",
    "    acc_scores_type.append(acc_type)\n",
    "    acc_scores_urgency.append(acc_urg)\n",
    "\n",
    "    print(f\"Referral Type → F1: {f1_type:.4f}, Acc: {acc_type:.4f}\")\n",
    "    print(f\"Urgency       → F1: {f1_urg:.4f}, Acc: {acc_urg:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nFinal kNN Results\")\n",
    "print(\"Referral Type F1:\", f1_scores_type)\n",
    "print(\"Referral Type Acc:\", acc_scores_type)\n",
    "print(f\"Mean F1: {np.mean(f1_scores_type):.4f}, Mean Acc: {np.mean(acc_scores_type):.4f}\")\n",
    "\n",
    "print(\"\\nUrgency F1:\", f1_scores_urgency)\n",
    "print(\"Urgency Acc:\", acc_scores_urgency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b746d",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8152573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = universal_ML_model(RandomForestClassifier, n_estimators=250, random_state=1)\n",
    "evaluate_multi_output_model(RF, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1b108f",
   "metadata": {},
   "source": [
    "### 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fecbd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results\n",
    "f1_scores_type = []\n",
    "f1_scores_urgency = []\n",
    "acc_scores_type = []\n",
    "acc_scores_urgency = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "    fold += 1\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Train your pipeline\n",
    "    RF.fit(X_train, y_train)\n",
    "    y_pred = RF.predict(X_test)\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=y.columns)\n",
    "\n",
    "    # Compute F1 and Accuracy per output\n",
    "    f1_type = f1_score(y_test['referal_type'], y_pred_df['referal_type'], average='macro')\n",
    "    f1_urg = f1_score(y_test['urgency'], y_pred_df['urgency'], average='macro')\n",
    "    acc_type = accuracy_score(y_test['referal_type'], y_pred_df['referal_type'])\n",
    "    acc_urg = accuracy_score(y_test['urgency'], y_pred_df['urgency'])\n",
    "\n",
    "    f1_scores_type.append(f1_type)\n",
    "    f1_scores_urgency.append(f1_urg)\n",
    "    acc_scores_type.append(acc_type)\n",
    "    acc_scores_urgency.append(acc_urg)\n",
    "\n",
    "    print(f\"Referral Type → F1: {f1_type:.4f}, Acc: {acc_type:.4f}\")\n",
    "    print(f\"Urgency       → F1: {f1_urg:.4f}, Acc: {acc_urg:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nFinal RF Results\")\n",
    "print(\"Referral Type F1:\", f1_scores_type)\n",
    "print(\"Referral Type Acc:\", acc_scores_type)\n",
    "print(f\"Mean F1: {np.mean(f1_scores_type):.4f}, Mean Acc: {np.mean(acc_scores_type):.4f}\")\n",
    "\n",
    "print(\"\\nUrgency F1:\", f1_scores_urgency)\n",
    "print(\"Urgency Acc:\", acc_scores_urgency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a496d",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7071f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = universal_ML_model(\n",
    "    MLPClassifier,\n",
    "    hidden_layer_sizes=(100,50,25),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    random_state=1\n",
    ")\n",
    "evaluate_multi_output_model(MLP, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3da9e4c",
   "metadata": {},
   "source": [
    "### 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results\n",
    "f1_scores_type = []\n",
    "f1_scores_urgency = []\n",
    "acc_scores_type = []\n",
    "acc_scores_urgency = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    print(f\"\\nFold {fold}\")\n",
    "    fold += 1\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Train your pipeline\n",
    "    MLP.fit(X_train, y_train)\n",
    "    y_pred = MLP.predict(X_test)\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=y.columns)\n",
    "\n",
    "    # Compute F1 and Accuracy per output\n",
    "    f1_type = f1_score(y_test['referal_type'], y_pred_df['referal_type'], average='macro')\n",
    "    f1_urg = f1_score(y_test['urgency'], y_pred_df['urgency'], average='macro')\n",
    "    acc_type = accuracy_score(y_test['referal_type'], y_pred_df['referal_type'])\n",
    "    acc_urg = accuracy_score(y_test['urgency'], y_pred_df['urgency'])\n",
    "\n",
    "    f1_scores_type.append(f1_type)\n",
    "    f1_scores_urgency.append(f1_urg)\n",
    "    acc_scores_type.append(acc_type)\n",
    "    acc_scores_urgency.append(acc_urg)\n",
    "\n",
    "    print(f\"Referral Type → F1: {f1_type:.4f}, Acc: {acc_type:.4f}\")\n",
    "    print(f\"Urgency       → F1: {f1_urg:.4f}, Acc: {acc_urg:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nFinal MLP Results\")\n",
    "print(\"Referral Type F1:\", f1_scores_type)\n",
    "print(\"Referral Type Acc:\", acc_scores_type)\n",
    "print(f\"Mean F1: {np.mean(f1_scores_type):.4f}, Mean Acc: {np.mean(acc_scores_type):.4f}\")\n",
    "\n",
    "print(\"\\nUrgency F1:\", f1_scores_urgency)\n",
    "print(\"Urgency Acc:\", acc_scores_urgency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345aa747",
   "metadata": {},
   "source": [
    "# Vascular-Only Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24319da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['referal_type'] != 'non vascular']\n",
    "df['referal_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713964d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#must be converted to strings so they cna be stored in the same array\n",
    "df['referal_type'] = df['referal_type'].astype(str)\n",
    "df['urgency'] = df['urgency'].astype(str)\n",
    "\n",
    "#Split the data\n",
    "X = df['letter_text']\n",
    "y = df[['referal_type', 'urgency']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25,\n",
    "                                                    random_state = 1)\n",
    "print(np.unique(y_test['referal_type']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfd4332",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the multi output model\n",
    "SVM = universal_ML_model(SVC, kernel = 'linear', probability = True, random_state = 1)\n",
    "evaluate_multi_output_model(SVM, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9877f",
   "metadata": {},
   "source": [
    "### 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a08d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results\n",
    "f1_scores_type = []\n",
    "f1_scores_urgency = []\n",
    "acc_scores_type = []\n",
    "acc_scores_urgency = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    print(f\"\\nFold {fold}\")\n",
    "    fold += 1\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Train your pipeline\n",
    "    SVM.fit(X_train, y_train)\n",
    "    y_pred = SVM.predict(X_test)\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=y.columns)\n",
    "\n",
    "    # Compute F1 and Accuracy per output\n",
    "    f1_type = f1_score(y_test['referal_type'], y_pred_df['referal_type'], average='macro')\n",
    "    f1_urg = f1_score(y_test['urgency'], y_pred_df['urgency'], average='macro')\n",
    "    acc_type = accuracy_score(y_test['referal_type'], y_pred_df['referal_type'])\n",
    "    acc_urg = accuracy_score(y_test['urgency'], y_pred_df['urgency'])\n",
    "\n",
    "    f1_scores_type.append(f1_type)\n",
    "    f1_scores_urgency.append(f1_urg)\n",
    "    acc_scores_type.append(acc_type)\n",
    "    acc_scores_urgency.append(acc_urg)\n",
    "\n",
    "    print(f\"Referral Type → F1: {f1_type:.4f}, Acc: {acc_type:.4f}\")\n",
    "    print(f\"Urgency       → F1: {f1_urg:.4f}, Acc: {acc_urg:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nFinal SVM Results\")\n",
    "print(\"Referral Type F1:\", f1_scores_type)\n",
    "print(\"Referral Type Acc:\", acc_scores_type)\n",
    "print(f\"Mean F1: {np.mean(f1_scores_type):.4f}, Mean Acc: {np.mean(acc_scores_type):.4f}\")\n",
    "\n",
    "print(\"\\nUrgency F1:\", f1_scores_urgency)\n",
    "print(\"Urgency Acc:\", acc_scores_urgency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188df912",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d35fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kNN = universal_ML_model(KNeighborsClassifier, n_neighbors = 5)\n",
    "evaluate_multi_output_model(kNN, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12769c96",
   "metadata": {},
   "source": [
    "### 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc14afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results\n",
    "f1_scores_type = []\n",
    "f1_scores_urgency = []\n",
    "acc_scores_type = []\n",
    "acc_scores_urgency = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    print(f\"\\nFold {fold}\")\n",
    "    fold += 1\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Train your pipeline\n",
    "    kNN.fit(X_train, y_train)\n",
    "    y_pred = kNN.predict(X_test)\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=y.columns)\n",
    "\n",
    "    # Compute F1 and Accuracy per output\n",
    "    f1_type = f1_score(y_test['referal_type'], y_pred_df['referal_type'], average='macro')\n",
    "    f1_urg = f1_score(y_test['urgency'], y_pred_df['urgency'], average='macro')\n",
    "    acc_type = accuracy_score(y_test['referal_type'], y_pred_df['referal_type'])\n",
    "    acc_urg = accuracy_score(y_test['urgency'], y_pred_df['urgency'])\n",
    "\n",
    "    f1_scores_type.append(f1_type)\n",
    "    f1_scores_urgency.append(f1_urg)\n",
    "    acc_scores_type.append(acc_type)\n",
    "    acc_scores_urgency.append(acc_urg)\n",
    "\n",
    "    print(f\"Referral Type → F1: {f1_type:.4f}, Acc: {acc_type:.4f}\")\n",
    "    print(f\"Urgency       → F1: {f1_urg:.4f}, Acc: {acc_urg:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nFinal kNN Results\")\n",
    "print(\"Referral Type F1:\", f1_scores_type)\n",
    "print(\"Referral Type Acc:\", acc_scores_type)\n",
    "print(f\"Mean F1: {np.mean(f1_scores_type):.4f}, Mean Acc: {np.mean(acc_scores_type):.4f}\")\n",
    "\n",
    "print(\"\\nUrgency F1:\", f1_scores_urgency)\n",
    "print(\"Urgency Acc:\", acc_scores_urgency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104df7fe",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = universal_ML_model(RandomForestClassifier, n_estimators=250, random_state=1)\n",
    "evaluate_multi_output_model(RF, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c0afa1",
   "metadata": {},
   "source": [
    "### 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results\n",
    "f1_scores_type = []\n",
    "f1_scores_urgency = []\n",
    "acc_scores_type = []\n",
    "acc_scores_urgency = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "    fold += 1\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Train your pipeline (assumes SVM is already a full pipeline)\n",
    "    RF.fit(X_train, y_train)\n",
    "    y_pred = RF.predict(X_test)\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=y.columns)\n",
    "\n",
    "    # Compute F1 and Accuracy per output\n",
    "    f1_type = f1_score(y_test['referal_type'], y_pred_df['referal_type'], average='macro')\n",
    "    f1_urg = f1_score(y_test['urgency'], y_pred_df['urgency'], average='macro')\n",
    "    acc_type = accuracy_score(y_test['referal_type'], y_pred_df['referal_type'])\n",
    "    acc_urg = accuracy_score(y_test['urgency'], y_pred_df['urgency'])\n",
    "\n",
    "    f1_scores_type.append(f1_type)\n",
    "    f1_scores_urgency.append(f1_urg)\n",
    "    acc_scores_type.append(acc_type)\n",
    "    acc_scores_urgency.append(acc_urg)\n",
    "\n",
    "    print(f\"Referral Type → F1: {f1_type:.4f}, Acc: {acc_type:.4f}\")\n",
    "    print(f\"Urgency       → F1: {f1_urg:.4f}, Acc: {acc_urg:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n=== Final SVM Results ===\")\n",
    "print(\"Referral Type F1:\", f1_scores_type)\n",
    "print(\"Referral Type Acc:\", acc_scores_type)\n",
    "print(f\"Mean F1: {np.mean(f1_scores_type):.4f}, Mean Acc: {np.mean(acc_scores_type):.4f}\")\n",
    "\n",
    "print(\"\\nUrgency F1:\", f1_scores_urgency)\n",
    "print(\"Urgency Acc:\", acc_scores_urgency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df9345",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90fbfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = universal_ML_model(\n",
    "    MLPClassifier,\n",
    "    hidden_layer_sizes=(100,50,25),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    random_state=1\n",
    ")\n",
    "evaluate_multi_output_model(MLP, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a346f609",
   "metadata": {},
   "source": [
    "### 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23b6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results\n",
    "f1_scores_type = []\n",
    "f1_scores_urgency = []\n",
    "acc_scores_type = []\n",
    "acc_scores_urgency = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "    fold += 1\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Train your pipeline (assumes SVM is already a full pipeline)\n",
    "    MLP.fit(X_train, y_train)\n",
    "    y_pred = MLP.predict(X_test)\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=y.columns)\n",
    "\n",
    "    # Compute F1 and Accuracy per output\n",
    "    f1_type = f1_score(y_test['referal_type'], y_pred_df['referal_type'], average='macro')\n",
    "    f1_urg = f1_score(y_test['urgency'], y_pred_df['urgency'], average='macro')\n",
    "    acc_type = accuracy_score(y_test['referal_type'], y_pred_df['referal_type'])\n",
    "    acc_urg = accuracy_score(y_test['urgency'], y_pred_df['urgency'])\n",
    "\n",
    "    f1_scores_type.append(f1_type)\n",
    "    f1_scores_urgency.append(f1_urg)\n",
    "    acc_scores_type.append(acc_type)\n",
    "    acc_scores_urgency.append(acc_urg)\n",
    "\n",
    "    print(f\"Referral Type → F1: {f1_type:.4f}, Acc: {acc_type:.4f}\")\n",
    "    print(f\"Urgency       → F1: {f1_urg:.4f}, Acc: {acc_urg:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n=== Final SVM Results ===\")\n",
    "print(\"Referral Type F1:\", f1_scores_type)\n",
    "print(\"Referral Type Acc:\", acc_scores_type)\n",
    "print(f\"Mean F1: {np.mean(f1_scores_type):.4f}, Mean Acc: {np.mean(acc_scores_type):.4f}\")\n",
    "\n",
    "print(\"\\nUrgency F1:\", f1_scores_urgency)\n",
    "print(\"Urgency Acc:\", acc_scores_urgency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
